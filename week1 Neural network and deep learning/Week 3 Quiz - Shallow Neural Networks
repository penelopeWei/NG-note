https://blog.csdn.net/u013733326/article/details/79866913


2.tanh激活函数通常比隐藏层单元的sigmoid激活函数效果更好，因为其输出的平均值更接近于零，因此它将数据集中在下一层是更好的选择，请问正确吗？

	1.True
	2.False

3.其中哪一个是第l层向前传播的正确向量化实现，其中1≤l≤L

	
	1.Z[l]=W[l]A[l−1]+b[l]
	2.A[l]=g[l](Z[l])
	主注：我只列出了正确的答案。

4.您正在构建一个识别黄瓜（y = 1）与西瓜（y = 0）的二元分类器。 你会推荐哪一种激活函数用于输出层？

	1.ReLU
	2.Leaky ReLU
	3.sigmoid
	4.tanh

5.下面的代码：
	A = np.random.randn(4,3)
	B = np.sum(A, axis = 1, keepdims = True)
	请问B.shape的值是多少?

6.假设你已经建立了一个神经网络。 您决定将权重和偏差初始化为零。 以下哪项陈述是正确的？
	1.第一个隐藏层中的每个神经元节点将执行相同的计算。 所以即使经过多次梯度下降迭代后，层中的每个神经元节点都会计算出与其他神经元节点相同的东西。
	2.第一个隐藏层中的每个神经元将在第一次迭代中执行相同的计算。 但经过一次梯度下降迭代后，他们将学会计算不同的东西，因为我们已经“破坏了对称性”。
	3.第一个隐藏层中的每一个神经元都会计算出相同的东西，但是不同层的神经元会计算不同的东西，因此我们已经完成了“对称破坏”。
	4.即使在第一次迭代中，第一个隐藏层的神经元也会执行不同的计算， 他们的参数将以自己的方式不断发展。

7.Logistic回归的权重w应该随机初始化，而不是全零，因为如果初始化为全零，那么逻辑回归将无法学习到有用的决策边界，因为它将无法“破坏对称性”，是正确的吗？
	1.True
	2.False

8.您已经为所有隐藏单元使用tanh激活建立了一个网络。 使用np.random.randn（..，..）* 1000将权重初始化为相对较大的值。 会发生什么？
	1.这没关系。只要随机初始化权重，梯度下降不受权重大小的影响。
	2.这将导致tanh的输入也非常大，因此导致梯度也变大。因此，您必须将α设置得非常小以防止发散; 这会减慢学习速度。
	3.这会导致tanh的输入也非常大，导致单位被“高度激活”，从而加快了学习速度，而权重必须从小数值开始。
	4.这将导致tanh的输入也很大，因此导致梯度接近于零， 优化算法将因此变得缓慢。


答案：
2.True
4.sigmoid,sigmoid函数主要用于二分类的输出层，Sigmoid输出的值介于0和1之间，这使其成为二元分类的一个非常好的选择。 如果输出小于0.5，则可以将其归类为0，如果输出大于0.5，则归类为1。 它也可以用tanh来完成，但是它不太方便，因为输出在-1和1之间。
5.B.shape = (4, 1).我们使用（keepdims = True）来确保A.shape是（4,1）而不是（4，），它使我们的代码更加严格。
6.答案是1
7.False.
   Logistic回归没有隐藏层。 如果将权重初始化为零，则Logistic回归中的第一个示例x将输出零，但Logistic回归的导数取决于不是零的输入x（因为没有隐藏层）。 因此，在第二次迭代中，如果x不是常量向量，则权值遵循x的分布并且彼此不同。
   
	若权重都为0,根据对称性，隐藏层对输出层的影响也会是一样，这就导致无论迭代隐藏层都在计算与彼此相同的函数。
8.答案4.因为权重初始化的值很大时，相当于位于tanh函数的右半部分，权重越大处于的tanh函数越平坦，梯度也越小，所以学习的速度也会非常慢



  



